{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_extension(path):\n",
    "  os.chdir(f' ...  type in your folder path my boy ... /{path}')\n",
    "  for p in os.listdir():\n",
    "    p_name, p_text = (os.path.splitext(p))\n",
    "    if p_text == '.jfif':\n",
    "      os.rename(p, p_name + '.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_extension('train/... son, you know what to do ... ') # incase you need to rename extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data/train/' # images to train goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(analysis):\n",
    "    analysis.sort() # sorts alphabetically\n",
    "    # This dictionary maps labels to integers, and the reverse\n",
    "    mapped_class = {analysis[i]:i for i in range(len(analysis))}\n",
    "    inv_mapped_class = {i:analysis[i] for i in range(len(analysis))}\n",
    "    return (mapped_class, inv_mapped_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(value, mapped_class):\n",
    "    encoding = zeros(len(mapped_class), dtype ='uint8')\n",
    "    encoding[mapped_class[value]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all(directory_string):\n",
    "    image_folder = listdir(directory_string)\n",
    "    len_folder  = len(image_folder)\n",
    "    \n",
    "    print(\"DETAILS : \")\n",
    "    print(\"\")\n",
    "    \n",
    "    if len_folder > 1:\n",
    "        print(f'There are {len_folder} classes in this directory :')\n",
    "    elif len_folder == 1:\n",
    "            print(f'There is {len_folder} class in this directory:')\n",
    "    elif len_folder == 0:\n",
    "            print(f'There are {len_folder} classes in this directory.')\n",
    "    \n",
    "    print(\" \")\n",
    "            \n",
    "    analysis = list()       \n",
    "    for sub_file in image_folder:\n",
    "        sub_file_num = listdir(directory_string+sub_file) \n",
    "        print(f'{sub_file} : {len(sub_file_num)} images.')\n",
    "        \n",
    "        analysis.append(sub_file)\n",
    "        \n",
    "    mapped_class, _ = mapper(analysis)\n",
    "    \n",
    "    print(' ')\n",
    "    \n",
    "    print('Preparing data for categorical classification ...')\n",
    "    \n",
    "    pictures, targets = list(), list()\n",
    "    for folder in image_folder:\n",
    "        current_directory = listdir(directory_string + folder)\n",
    "        for picture in current_directory :\n",
    "            if picture != 'Thumbs.db':\n",
    "                \n",
    "                # load the picture\n",
    "                photo = load_img(directory_string + folder + '/'+ picture, target_size =(224,224))\n",
    "                \n",
    "                #convert to numpy array\n",
    "                photo = img_to_array(photo, dtype='uint8')  \n",
    "                \n",
    "                \n",
    "                # one_hot_encode the targets\n",
    "                target = one_hot_encoder(folder, mapped_class)\n",
    "            \n",
    "                # append to list\n",
    "                pictures.append(photo)\n",
    "                targets.append(target)\n",
    "    \n",
    "    X = asarray(pictures, dtype = 'uint8')\n",
    "    Y = asarray(targets, dtype = 'uint8')\n",
    "    \n",
    "    print(' ')\n",
    "    print(\"Done!\")\n",
    "    return X, Y\n",
    " # dagashi kashi  <--- download later (anime about programming)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = collect_all(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save both arrays to one file in compressed format\n",
    "savez_compressed('clean_data.npz', X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f beta metric, incase of imbalanced dataset ...... \n",
    "def f_beta(y_true, y_pred, beta =2):\n",
    "    # clip prediction\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    \n",
    "    # calculate elements\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0,1)), axis = 1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0,1)), axis = 1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0,1)), axis = 1)\n",
    "    \n",
    "    #  precision\n",
    "    p = tp / (tp + fp + backend.epsilon())\n",
    "    \n",
    "    #  recall\n",
    "    r = tp / (tp +fn + backend.epsilon())\n",
    "    \n",
    "    # calculate fbeta\n",
    "    \n",
    "    bb = beta ** 2\n",
    "    fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n",
    "    return fbeta_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(input_shape =(224,224,3), output_shape = 3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(16,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(32,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(64,(3,3), activation ='relu', kernel_initializer ='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer ='he_uniform'))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01,  momentum = 0.9)\n",
    "    model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])# metrics=[f_beta])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(info):\n",
    "    # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot (info.history['loss'], color='blue', label='train')\n",
    "    plt.plot (info.history['val_loss'], color='orange', label='test')\n",
    "    plt.legend([\"Loss\",\"Validation Loss\"])\n",
    "    \n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot (info.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot (info.history['val_accuracy'], color='orange', label='test')\n",
    "    plt.legend([\"Accuracy\",\"Validation Accuracy\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_dataset(X,Y):\n",
    "    train_x, test_x,train_y,test_y = train_test_split(X, Y, test_size = 0.2, random_state =3)\n",
    "    print(train_x.shape, test_x.shape,train_y.shape,test_y.shape)\n",
    "    return(train_x, test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning_sequence(X,Y):\n",
    "    # load dataset\n",
    "    train_x, test_x,train_y,test_y = load_split_dataset(X, Y)\n",
    "    \n",
    "    #create data generator\n",
    "    train_datagen = ImageDataGenerator(rescale =1.0/255.0,  horizontal_flip = True, vertical_flip = True, rotation_range = 90)\n",
    "    val_datagen = ImageDataGenerator(rescale =1.0/255.0)\n",
    "    \n",
    "    # prepare iterations\n",
    "    train = train_datagen.flow(train_x, train_y, batch_size = 32)\n",
    "    val = val_datagen.flow(test_x, test_y, batch_size = 32)\n",
    "        \n",
    "    # define the model\n",
    "    model = my_model()\n",
    "    \n",
    "    # fitting the model\n",
    "    info = model.fit_generator(train, steps_per_epoch = 90, validation_data = val,      # steps_per_epoch = len(train), validation_steps = len(val)\n",
    "                                 validation_steps = 15, epochs = 120, verbose = 1)      # or steps_per_epoch = 879/30 => 29.3\n",
    "                                                                                        # round up or round down, you decide.\n",
    "                                                                                        # 879 is the total number of images\n",
    "                                                                                        # in the train dataset, and 30 is the\n",
    "                                                                                        # batch_size declared in train_datagen.flow()\n",
    "    \n",
    "    # evaluate the model                                                                # steps = len(val)              \n",
    "    loss, accuracy = model.evaluate_generator(val, steps = 15, verbose = 0)\n",
    "    print('> loss = %.3f, accuracy=%.3f' %(loss, accuracy))\n",
    "    \n",
    "     # saving the model\n",
    "    model.save('EITS_model.h5') #EITS => Eye In The Sky model ... hehehe\n",
    "    \n",
    "    # display learning curves\n",
    "    summary(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_learning_sequence(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_string = 'data/train/' # yes yes, i know this is the train set. I just need the inverse tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = list()\n",
    "image_folder = listdir(directory_string)\n",
    "for sub_file in image_folder:\n",
    "    analysis.append(sub_file)\n",
    "        \n",
    "_, inv_mapped_class = mapper(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapped_class # See for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_labels(inv_mapping, prediction):\n",
    "    value  = prediction.round()\n",
    "    # convert to predicted tags\n",
    "    labels = [inv_mapping[i] for i in range(len(value)) if value[i] == 1.0]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The approach I would take is to convert all the test images into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'data/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_image(directory):\n",
    "    test_pictures = list()\n",
    "    print('Converting to numpy array ...')\n",
    "    for picture in listdir(directory) :\n",
    "         if picture != 'Thumbs.db':\n",
    "                 # load the picture\n",
    "                photo = load_img(directory + picture, target_size = (224,224))\n",
    "                \n",
    "                #convert to numpy array\n",
    "                photo = img_to_array(photo, dtype='uint8')  \n",
    "                \n",
    "                photo = photo.reshape(1, 224, 224, 3)\n",
    "                \n",
    "                 # append to list\n",
    "                test_pictures.append(photo)\n",
    "                \n",
    "    X_TEST = asarray(test_pictures, dtype = 'uint8')  \n",
    "    print(' ')\n",
    "    print('Done!')\n",
    "    return X_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_sequence(X_TEST, inv_mapped_class):\n",
    "    # load model\n",
    "    model = load_model('EITS_model.h5')\n",
    "    \n",
    "    # predict the class\n",
    "    for i in range(len(X_TEST)):\n",
    "        result = model.predict((X_TEST[i]))\n",
    "        #print(result[0])\n",
    "        # map predictions to tags\n",
    "        label = prediction_to_labels(inv_mapped_class, result[0])\n",
    "        print(label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_prediction_sequence(X_TEST, inv_mapped_class)  # So yeah, that's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
