{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PREPROCESSING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The preprocessing function accepts a pandas dataframe with the \"Year\", \"Month\" and \"Day\" features for when a natural disaster\n# occured as well as the \"Longitude\" and \"latitude\" coordinates. The function returns a pandas dataframe with the mentioned \n# features but for all days within the starting and ending year of the original dataframe. It also has an added feature \n# \"target\" which is a 1 if a natural disaster occured on that day, and a 0 otherwise.\n\ndef preprocessing_dataframe(disaster_df):\n    preprocessed_dict = {'Year': [], 'Latitude':[], 'Longitude': [], 'Month': [], 'Day': [], 'target': []} # Starting with a dictionary to hold all values, but will later change to a pandas dataframe\n    # Creating a dictionary that stores the latitude and longitude values for each specific place in the dataframe\n    print('Preprocessing ... ')\n    print(' ')\n    place_coords = {}\n    for place in disaster_df['Name'].unique():\n        lat = disaster_df[disaster_df['Name'] == place]['Latitude'].unique()[0]\n        lng = disaster_df[disaster_df['Name'] == place]['Longitude'].unique()[0]\n        place_coords[place] = (lat, lng)\n        \n    # All places with their respective coordinates are now stored in the \"place_coords\" dictionary\n    \n    year_start = disaster_df['Year'].unique().min() # Getting the earliest year in the dataframe\n    year_end = disaster_df['Year'].unique().max() # Getting the last year in the dataframe\n    \n    # Now, I'll iterate through all the years in order to assign the targets\n    for year in range(year_start, year_end+1):  \n        year_df = disaster_df[disaster_df['Year'] == year] # Dataframe for disasters happening in year \"year\" \n        \n        # I'll have to account for all the days of the months in the year, which are usually 30 and 31 except February\n        # Assigning the number of days for a specific year in the month of February is dependent on if the year is a leap year \n        # or not, where the number of days will be 29 or 28 respectively.\n        \n        month_days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] # List containing number of days for each month of the year accordingly, i.e index 0 or January with 31 days. This is the list of days assuming it is not a leap year\n        if year%4 == 0:\n            if year%100 != 0:\n                month_days[1] = 29\n            else:\n                if year%400 == 0:\n                    month_days[1] = 29\n        \n        # Now, the \"month_days\" list's index \"1\" will remain 28 if it is not a leap year, and be changed to 29 if it is indeed\n        # a leap year\n        \n        # Would also need to iterate through all the places in the dataframe\n        for place in place_coords:\n            place_df = year_df[year_df['Name'] == place] # DataFrame for observations of only the place \"place\" \n            month_number = 1 # This is supposed to be January\n            \n            #Similar, iterating through all months...\n            for days in month_days:\n                month_df = place_df[place_df['Month'] == month_number] # DataFrame containing observations of only the month \"month\"\n                \n                # Iterating through all the days in the month...\n                for day in range(1, days+1):\n                    preprocessed_dict['Year'].append(year)\n                    preprocessed_dict['Latitude'].append(place_coords[place][0])\n                    preprocessed_dict['Longitude'].append(place_coords[place][1])\n                    preprocessed_dict['Month'].append(month_number)\n                    preprocessed_dict['Day'].append(day)\n                    # And finally, if the particular date is present in the dataframe, the target is set to 1, and 0 otherwise\n                    if place in year_df['Name'].unique() and month_number in place_df['Month'].unique() and day in month_df['Day'].unique():\n                        preprocessed_dict['target'].append(1)\n                    else:\n                        preprocessed_dict['target'].append(0)\n                month_number += 1\n                \n    preprocessed_df = pd.DataFrame(preprocessed_dict) # Transforming to a dataframe\n    \n    # Things to note: The function doesn't consider nan values, so if there is a nan value in any of the date features it will\n    # set the target to 0. Also, the preprocessed dataframe can be very large without care, so maybe sticking to 40, 50 years\n    # at most will be desirable. Also helps that for latter years, there's a lot less nan values. But could also edit it to\n    # perform a task if there is are nan values present.\n    print('Done!')\n    return preprocessed_df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in the csv txt file\ntsu = pd.read_csv('/kaggle/input/tsunami/tsrunup.txt',delimiter = '\\t', quoting = 3, encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the first 5\ntsu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for null values\ntsu.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taking the features to used for classification\ntsu  = tsu[['DAY','MONTH','YEAR', 'LOCATION_NAME','COUNTRY','LATITUDE','LONGITUDE']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After checking on what year to start from, it turns out that from 2000 to present times, has the fewest amount NaN values.\n# In order not to mess with the data i would be removing all nan cells\nchecking = tsu[tsu['YEAR'] >= 2000]\nchecking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of new dataframe\nchecking.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resetting index\nchecking.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for null cells\nchecking.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping all rows with null cells\nchecking = checking.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for duplicates\nchecking.duplicated().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping all duplicates\nchecking.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"checking.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resetting index\nchecking.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq = checking['LOCATION_NAME'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"see = checking['LOCATION_NAME'].value_counts()\nsee[:50] #Displaying all the values in  the variable see, it can be noted that there are someplaces that a tsunami occured only once.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# analyzing the data, checking to see what the model would train on and if they are relevant. And possibly removing \n# places that a tsunami occurred a few number of times in a couple of years.\nfor uni in uniq:\n    if checking['LOCATION_NAME'].value_counts()[uni] <= 5:\n        chi = checking[checking['LOCATION_NAME'] == uni]\n        print(chi)\n        #chi = chi.values.tolist()    converting to pandaframe to lists","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Removing places with 5 or less occurences since 2000\nfor i in range(len(checking)):\n    if see[checking.loc[i, 'LOCATION_NAME']] < 4:\n        checking.drop(i, axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checking.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resetting index\nchecking.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checking.rename(columns={'DAY':'Day',\n                          'MONTH':'Month',\n                          'YEAR':'Year',\n                         'LOCATION_NAME':'Name',\n                        'COUNTRY':'Country',\n                         'LATITUDE':'Latitude',\n                         'LONGITUDE':'Longitude'}, \n                 inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessed_tsu = preprocessing_dataframe(checking)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessed_tsu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessed_tsu.to_csv('/kaggle/input/tsunami-clean/tsunami_classification_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tsu = pd.read_csv('/kaggle/input/tsunami-clean/tsunami_classification_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tsu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tsu.drop(['Unnamed: 0'],axis = 1, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tsu.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tsu.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(preprocessed_tsu[preprocessed_tsu['target'] == 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(preprocessed_tsu[preprocessed_tsu['target'] == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minority = preprocessed_tsu[preprocessed_tsu['target'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minority.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"majority = preprocessed_tsu[preprocessed_tsu['target'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"majority .head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n# Upsample minority class\nmaj_downsamp = resample(majority , replace=True, n_samples=1900, random_state=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downsampled_data = pd.concat([minority,maj_downsamp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downsampled_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downsampled_data = downsampled_data.sample(frac=1).reset_index(drop=True) # to shuffle the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downsampled_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downsampled_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = downsampled_data.iloc[:, 0:5]\ny = downsampled_data.iloc[:, 5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier= GaussianNB()\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\ncm = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check = classification_report(y_test, y_pred)\nprint(check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# Predict class probabilities\ny_pred1 = classifier.predict_proba(x_test)\n \n# Keep only the positive class\ny_pred1 = [y[1] for y in y_pred1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(y_test, y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier1 = RandomForestClassifier(n_estimators=10, criterion='entropy' , random_state=0) # Increased to 30 estimators\nclassifier1.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predR = classifier1.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_1 = confusion_matrix(y_test, y_predR)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_predR, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check = classification_report(y_test, y_predR)\nprint(check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict class probabilities\ny_predR2 = classifier1.predict_proba(x_test)\n \n# Keep only the positive class\ny_predR2 = [y[1] for y in y_predR2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(y_test, y_predR2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gzip\nimport dill\n\n# serializing using dill\nwith gzip.open('natural_disaster_Tsunami_Classification.dill.gz', 'wb') as f:\n    dill.dump(classifier1, f, recurse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('TsuClass_Model.dill', 'wb') as f:\n    dill.dump(classifier1, f, recurse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}